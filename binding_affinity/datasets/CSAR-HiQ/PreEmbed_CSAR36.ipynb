{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62fbf377-feca-4f8c-9bff-72bdad11b273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Load data ...\n",
      "2. Load pretrained ESM2 model ...\n",
      "3. Compute (physchem + ESM) per residue and concat along L ...\n",
      "4. Save to pickle ...\n",
      "✅ Done. Saved: /workspace/binding_affinity/datasets/generalization/36_complex.pkl\n"
     ]
    }
   ],
   "source": [
    "INPUT_TSV  = \"/workspace/binding_affinity/datasets/CSAR-HiQ/CSAR36.tsv\"   \n",
    "OUTPUT_PKL = \"/workspace/binding_affinity/datasets/CSAR-HiQ/36_complex.pkl\"\n",
    "USE_LABELS = True  \n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import esm\n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + \"/opt/conda/envs/team05/bin\"\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"  \n",
    "\n",
    "AMINO_ACIDS = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "charge_dict = {\"D\": -1, \"E\": -1, \"K\": 1, \"R\": 1, \"H\": 1}\n",
    "polar_dict = {\"S\": 1, \"T\": 1, \"N\": 1, \"Q\": 1, \"Y\": 1, \"C\": 1, \"D\": 1, \"E\": 1, \"K\": 1, \"R\": 1, \"H\": 1}\n",
    "hydro_dict = {\n",
    "    \"A\": 1.8, \"C\": 2.5, \"D\": -3.5, \"E\": -3.5, \"F\": 2.8,\n",
    "    \"G\": -0.4, \"H\": -3.2, \"I\": 4.5, \"K\": -3.9, \"L\": 3.8,\n",
    "    \"M\": 1.9, \"N\": -3.5, \"P\": -1.6, \"Q\": -3.5, \"R\": -4.5,\n",
    "    \"S\": -0.8, \"T\": -0.7, \"V\": 4.2, \"W\": -0.9, \"Y\": -1.3\n",
    "}\n",
    "weight_dict = {\n",
    "    \"A\": 89.1, \"C\": 121.2, \"D\": 133.1, \"E\": 147.1, \"F\": 165.2,\n",
    "    \"G\": 75.1, \"H\": 155.2, \"I\": 131.2, \"K\": 146.2, \"L\": 131.2,\n",
    "    \"M\": 149.2, \"N\": 132.1, \"P\": 115.1, \"Q\": 146.2, \"R\": 174.2,\n",
    "    \"S\": 105.1, \"T\": 119.1, \"V\": 117.1, \"W\": 204.2, \"Y\": 181.2\n",
    "}\n",
    "\n",
    "def clean_sequence(seq: str) -> str:\n",
    "    \"\"\"\n",
    "    Replace unsupported amino acids with 'X' (unknown).\n",
    "    ESM alphabet supports 20 AA + B, Z, X.\n",
    "    \"\"\"\n",
    "    allowed = set(\"ACDEFGHIKLMNPQRSTVWYBXZ\")\n",
    "    return \"\".join([aa if aa in allowed else \"X\" for aa in seq])\n",
    "\n",
    "def get_physchem_features(sequence: str) -> np.ndarray:\n",
    "    \"\"\"Return [L, 6] = [Acidic, Basic, Neutral, Polar, Hydrophobicity, MolWeight].\"\"\"\n",
    "    feats = []\n",
    "    for aa in sequence:\n",
    "        charge = charge_dict.get(aa, 0)\n",
    "        acidic  = 1 if charge == -1 else 0\n",
    "        basic   = 1 if charge == 1 else 0\n",
    "        neutral = 1 if charge == 0 else 0\n",
    "        polar   = polar_dict.get(aa, 0)\n",
    "        hydro   = float(hydro_dict.get(aa, 0.0))\n",
    "        weight  = float(weight_dict.get(aa, 0.0))\n",
    "        feats.append([acidic, basic, neutral, polar, hydro, weight])\n",
    "    return np.array(feats, dtype=np.float32)\n",
    "\n",
    "print(\"1. Load data ...\")\n",
    "df = pd.read_csv(INPUT_TSV, sep=\"\\t\")\n",
    "if USE_LABELS:\n",
    "    IDs = df.iloc[:, 0].astype(str).values\n",
    "    SEQS = df.iloc[:, 2].astype(str).values\n",
    "    BIND = df.iloc[:, 3].astype(str).values\n",
    "else:\n",
    "    IDs = df.iloc[:, 0].astype(str).values\n",
    "    SEQS = df.iloc[:, 2].astype(str).values\n",
    "\n",
    "print(\"2. Load pretrained ESM2 model ...\")\n",
    "model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device).eval()\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "\n",
    "print(\"3. Compute (physchem + ESM) per residue and concat along L ...\")\n",
    "feat_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for seq in SEQS:\n",
    "        chain_seqs = [s.strip() for s in seq.split(\",\") if s.strip()]\n",
    "        per_residue_blocks = []\n",
    "\n",
    "        for chain in chain_seqs:\n",
    "\n",
    "            chain = clean_sequence(chain)\n",
    "            data = [(\"protein\", chain)]\n",
    "            _, _, tokens = batch_converter(data)\n",
    "            tokens = tokens.to(device)\n",
    "\n",
    "            out = model(tokens, repr_layers=[33], return_contacts=False)\n",
    "            token_repr = out[\"representations\"][33]   \n",
    "            emb = token_repr[0, 1:-1].detach().cpu().numpy().astype(np.float32) \n",
    "\n",
    "            phys = get_physchem_features(chain)       \n",
    "\n",
    "            per_chain = np.concatenate([phys, emb], axis=1)\n",
    "            per_residue_blocks.append(per_chain)\n",
    "\n",
    "        combined = np.concatenate(per_residue_blocks, axis=0) if len(per_residue_blocks) > 1 else per_residue_blocks[0]\n",
    "        feat_list.append(combined)\n",
    "\n",
    "print(\"4. Save to pickle ...\")\n",
    "os.makedirs(os.path.dirname(OUTPUT_PKL), exist_ok=True)\n",
    "with open(OUTPUT_PKL, \"wb\") as f:\n",
    "    if USE_LABELS:\n",
    "        pickle.dump((IDs, SEQS, BIND, feat_list), f)\n",
    "    else:\n",
    "        pickle.dump((IDs, SEQS, feat_list), f)\n",
    "print(f\"✅ Done. Saved: {OUTPUT_PKL}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cd74833-c9ae-4402-804b-5897f363975c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저장 완료 → /workspace/binding_affinity/datasets/generalization/36_smi_features.pkl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "\n",
    "def compute_global_properties(smiles: str) -> list[float]:\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return [0.0] * 10\n",
    "    return [\n",
    "        Descriptors.MolWt(mol),\n",
    "        Descriptors.MolLogP(mol),\n",
    "        Descriptors.TPSA(mol),\n",
    "        Descriptors.NumRotatableBonds(mol),\n",
    "        Descriptors.HeavyAtomCount(mol),\n",
    "        Descriptors.FractionCSP3(mol),\n",
    "        Descriptors.NumHDonors(mol),\n",
    "        Descriptors.NumHAcceptors(mol),\n",
    "        Descriptors.RingCount(mol),\n",
    "        Descriptors.MolMR(mol),\n",
    "    ]\n",
    "\n",
    "input_csv  = \"/workspace/binding_affinity/datasets/CSAR36/CSAR36.tsv\"\n",
    "output_pkl = \"/workspace/binding_affinity/datasets/CSAR36/36_smi_features.pkl\"\n",
    "\n",
    "df = pd.read_csv(input_csv, sep=\"\\t\", engine=\"python\")\n",
    "\n",
    "if \"SMILES\" not in df.columns:\n",
    "    raise KeyError(f\"'SMILES' 컬럼이 없습니다. 현재 컬럼: {list(df.columns)[:10]} ...\")\n",
    "\n",
    "df[\"global_feat\"] = [compute_global_properties(smi) for smi in df[\"SMILES\"].tolist()]\n",
    "\n",
    "os.makedirs(os.path.dirname(output_pkl), exist_ok=True)\n",
    "df.to_pickle(output_pkl)\n",
    "print(f\"저장 완료 → {output_pkl}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb5bd1a1-2fad-49da-862f-231ce11a7299",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import os, re, ast, pickle\n",
    "from typing import Any, Dict, List, Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "PDB_RE = re.compile(r\"^[0-9][A-Za-z0-9]{3}$\")  \n",
    "\n",
    "def _parse_bs(text: Any, index_base: int, Lp: Optional[int] = None) -> np.ndarray:\n",
    "    if text is None or (isinstance(text, float) and np.isnan(text)):\n",
    "        return np.zeros((0,), dtype=np.int64)\n",
    "    if isinstance(text, (list, tuple, np.ndarray)):\n",
    "        arr = np.asarray(text, dtype=np.int64)\n",
    "    else:\n",
    "        parts = [t.strip() for t in str(text).split(\",\") if t.strip()]\n",
    "        arr = np.asarray([int(t) for t in parts], dtype=np.int64)\n",
    "    if index_base == 1:\n",
    "        arr = arr - 1\n",
    "    if Lp is not None:\n",
    "        arr = arr[(arr >= 0) & (arr < Lp)]\n",
    "    return arr.astype(np.int64)\n",
    "\n",
    "def _parse_global_feat(v: Any) -> np.ndarray:\n",
    "    if v is None:\n",
    "        return np.zeros((0,), dtype=np.float32)\n",
    "    if isinstance(v, (list, tuple, np.ndarray)):\n",
    "        return np.asarray(v, dtype=np.float32)\n",
    "    s = str(v).strip()\n",
    "    if not s:\n",
    "        return np.zeros((0,), dtype=np.float32)\n",
    "    try:\n",
    "        x = ast.literal_eval(s)\n",
    "    except Exception:\n",
    "        toks = [t for t in re.split(r\"[\\s,]+\", s.strip(\"[]\")) if t]\n",
    "        x = [float(t) for t in toks]\n",
    "    return np.asarray(x, dtype=np.float32)\n",
    "\n",
    "def _choose_target(row: dict|pd.Series, mode: str) -> float:\n",
    "    if mode == \"pAff\":\n",
    "        return float(row[\"pAff\"])\n",
    "    if mode == \"Affinity_nM\":\n",
    "        return float(row[\"Affinity_nM\"])\n",
    "    if mode == \"Affinity_nM_log10\":\n",
    "        nM = max(float(row[\"Affinity_nM\"]), 1e-12)\n",
    "        return float(9.0 - np.log10(nM))\n",
    "    raise ValueError(mode)\n",
    "\n",
    "def _load_complex_pkl(path: str, force_idx: tuple[int|None,int|None,int|None,int|None]=(None,None,None,None)) -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"complex.pkl(tuple-of-columns) → {PDB: {'prot_feats',(Lp,C) float32, 'prot_mask',(Lp,) bool}}\"\"\"\n",
    "    with open(path, \"rb\") as f:\n",
    "        obj = pickle.load(f)\n",
    "    if not isinstance(obj, (list, tuple)):\n",
    "        raise TypeError(f\"complex.pkl top-level must be tuple/list, got {type(obj).__name__}\")\n",
    "    cols = list(obj)\n",
    "    ncols = len(cols)\n",
    "    lengths = [len(c) if hasattr(c, \"__len__\") else None for c in cols]\n",
    "    valid_idx = [i for i,L in enumerate(lengths) if L is not None]\n",
    "    if not valid_idx:\n",
    "        raise ValueError(\"No list-like columns in complex.pkl\")\n",
    "    N = lengths[valid_idx[0]]\n",
    "    if not all(lengths[i] == N for i in valid_idx):\n",
    "        raise ValueError(f\"Columns have different lengths: {[lengths[i] for i in valid_idx]}\")\n",
    "\n",
    "    def kind_auto(col):\n",
    "        m = min(20, len(col))\n",
    "        cnt_id=cnt_feat2d=cnt_mask1d=cnt_seq=0\n",
    "        for i in range(m):\n",
    "            v = col[i]\n",
    "            if isinstance(v, str) and PDB_RE.match(v): cnt_id+=1\n",
    "            if isinstance(v, np.ndarray) and v.ndim==2 and np.issubdtype(v.dtype, np.number): cnt_feat2d+=1\n",
    "            if isinstance(v, np.ndarray) and v.ndim==1 and (v.dtype==bool or np.issubdtype(v.dtype, np.integer)):\n",
    "                x = v[:min(len(v),20)]\n",
    "                if x.size>0 and np.isin(np.unique(x), [0,1,True,False]).all(): cnt_mask1d+=1\n",
    "            if isinstance(v, str) and len(v)>=50: cnt_seq+=1\n",
    "        if cnt_id>=m*0.7: return \"ids\"\n",
    "        if cnt_feat2d>=m*0.7: return \"feats\"\n",
    "        if cnt_mask1d>=m*0.7: return \"mask\"\n",
    "        if cnt_seq>=m*0.7: return \"seq\"\n",
    "        return \"other\"\n",
    "\n",
    "    id_i, feat_i, mask_i, seq_i = force_idx\n",
    "    if id_i is None or feat_i is None:\n",
    "        kinds = [kind_auto(cols[i]) for i in range(ncols)]\n",
    "        if id_i   is None: id_i   = next((i for i,k in enumerate(kinds) if k==\"ids\"), None)\n",
    "        if feat_i is None: feat_i = next((i for i,k in enumerate(kinds) if k==\"feats\"), None)\n",
    "        if mask_i is None: mask_i = next((i for i,k in enumerate(kinds) if k==\"mask\"), None)\n",
    "\n",
    "    if id_i is None or feat_i is None:\n",
    "        raise ValueError(f\"Could not find ids/features in complex.pkl (force_idx={force_idx})\")\n",
    "\n",
    "    ids, feats_col = cols[id_i], cols[feat_i]\n",
    "    mask_col = cols[mask_i] if (mask_i is not None) else None\n",
    "\n",
    "    out = {}\n",
    "    for i in range(N):\n",
    "        pid   = str(ids[i]).strip()\n",
    "        feats = feats_col[i].astype(np.float32) if isinstance(feats_col[i], np.ndarray) else np.asarray(feats_col[i], dtype=np.float32)\n",
    "        if feats.ndim != 2:\n",
    "            raise ValueError(f\"[{pid}] features is not 2D: {feats.shape}\")\n",
    "        Lp = feats.shape[0]\n",
    "        if mask_col is not None and isinstance(mask_col[i], np.ndarray) and mask_col[i].ndim==1 and len(mask_col[i])==Lp:\n",
    "            pmask = mask_col[i].astype(bool)\n",
    "        else:\n",
    "            pmask = np.ones((Lp,), dtype=bool)\n",
    "        out[pid] = {\"prot_feats\": feats, \"prot_mask\": pmask}\n",
    "    return out\n",
    "\n",
    "def merge_to_train(\n",
    "    smi_pkl: str,\n",
    "    complex_pkl: str,\n",
    "    out_path: str,\n",
    "    *,\n",
    "    index_base: int = 0,\n",
    "    use_y: str = \"pAff\",\n",
    "    chemberta_model: str = \"seyonec/ChemBERTa-zinc-base-v1\",\n",
    "    max_len: int = 256,\n",
    "    force_complex_idx: tuple[int|None,int|None,int|None,int|None]=(None,None,None,None),\n",
    "):\n",
    "    df = pd.read_pickle(smi_pkl)\n",
    "    assert isinstance(df, pd.DataFrame), \"smi_pkl must be a DataFrame pkl\"\n",
    "    req = {\"PDB\",\"Sequence\",\"BS\",\"SMILES\",\"global_feat\"}\n",
    "    miss = req - set(df.columns)\n",
    "    if miss: raise KeyError(f\"{smi_pkl} missing columns: {sorted(miss)}\")\n",
    "\n",
    "    prot_map = _load_complex_pkl(complex_pkl, force_idx=force_complex_idx)\n",
    "\n",
    "    tokzr = AutoTokenizer.from_pretrained(chemberta_model, use_fast=True)\n",
    "    smiles_list = df[\"SMILES\"].astype(str).tolist()\n",
    "    tok = tokzr(smiles_list, padding=\"max_length\", truncation=True,\n",
    "                max_length=max_len, return_attention_mask=True, return_tensors=None)\n",
    "    all_ids, all_masks = tok[\"input_ids\"], tok[\"attention_mask\"]\n",
    "\n",
    "    samples, not_found = [], 0\n",
    "    for i, row in enumerate(df.itertuples(index=False)):\n",
    "        s = row._asdict()\n",
    "        pdb = str(s[\"PDB\"]).strip()\n",
    "        if pdb not in prot_map:\n",
    "            not_found += 1\n",
    "            continue\n",
    "\n",
    "        feats = prot_map[pdb][\"prot_feats\"]; pmask = prot_map[pdb][\"prot_mask\"]\n",
    "        Lp = feats.shape[0]\n",
    "        br_idx = _parse_bs(s[\"BS\"], index_base, Lp=Lp)\n",
    "        lig_ids = np.asarray(all_ids[i], dtype=np.int64)\n",
    "        lig_msk = np.asarray(all_masks[i], dtype=np.int64).astype(bool)\n",
    "        lig_glb = _parse_global_feat(s[\"global_feat\"]).astype(np.float32)\n",
    "        yval    = np.float32(_choose_target(s, use_y))\n",
    "\n",
    "        samples.append({\n",
    "            \"id\": pdb,\n",
    "            \"prot_feats\": feats.astype(np.float32),\n",
    "            \"prot_mask\":  pmask.astype(bool),\n",
    "            \"br_indices\": br_idx.astype(np.int64),\n",
    "            \"lig_token_ids\": lig_ids,\n",
    "            \"lig_mask\": lig_msk,\n",
    "            \"lig_global\": lig_glb,\n",
    "            \"y\": yval,\n",
    "        })\n",
    "\n",
    "    os.makedirs(os.path.dirname(out_path) or \".\", exist_ok=True)\n",
    "    with open(out_path, \"wb\") as f:\n",
    "        pickle.dump(samples, f)\n",
    "\n",
    "    kept = len(samples)\n",
    "    print(f\"Saved {kept} samples -> {out_path} (skipped {not_found} without protein features)\")\n",
    "    if kept:\n",
    "        Lp, C = samples[0][\"prot_feats\"].shape\n",
    "        Ll = samples[0][\"lig_token_ids\"].shape[0]\n",
    "        Fg = samples[0][\"lig_global\"].shape[0]\n",
    "        print(f\"Example shapes: prot_feats=({Lp},{C}), lig_len={Ll}, lig_global_dim={Fg}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d545d8e0-c854-45fc-b5fe-a69cf0c97048",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/team05/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 36 samples -> /workspace/binding_affinity/datasets/generalization/CSAR36.pkl (skipped 0 without protein features)\n",
      "Example shapes: prot_feats=(443,1286), lig_len=256, lig_global_dim=10\n"
     ]
    }
   ],
   "source": [
    "merge_to_train(\n",
    "    smi_pkl=\"/workspace/binding_affinity/datasets/CSAR36/36_smi_features.pkl\",\n",
    "    complex_pkl=\"/workspace/binding_affinity/datasets/CSAR36/36_complex.pkl\",\n",
    "    out_path=\"/workspace/binding_affinity/datasets/CSAR36/CSAR36.pkl\",\n",
    "    index_base=0,\n",
    "    use_y=\"pAff\",\n",
    "    chemberta_model=\"seyonec/ChemBERTa-zinc-base-v1\",\n",
    "    max_len=256,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d26848-19c3-4e7f-9870-927029b6546e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (team05)",
   "language": "python",
   "name": "team05"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
